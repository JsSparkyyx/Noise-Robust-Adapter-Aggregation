{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from evaluate import evaluator\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n",
    "from datasets import DatasetDict, load_dataset\n",
    "import numpy as np\n",
    "from init_parameters import init_parameters\n",
    "from data import split_data, set_seed, k_split\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'glue'\n",
    "task = 'sst2'\n",
    "seed = 42\n",
    "num_clients = 10\n",
    "num_error_clients = 3\n",
    "number = 5\n",
    "model_name_or_path = 'google/flan-t5-base'\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "set_seed(seed)\n",
    "if data_name == 'bigbench':\n",
    "    dataset = load_dataset(\"tasksource/bigbench\", task).shuffle(seed=seed)\n",
    "    dataset = dataset.rename_columns({'inputs':'source','targets':'target'})\n",
    "else:\n",
    "    dataset = load_dataset(\"JsSparkYyx/NLP524\", task).shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = k_split(num_clients,num_error_clients,dataset['train'])\n",
    "if data_name == 'glue':\n",
    "    test_ds = k_split(num_clients,num_error_clients,dataset['test'])\n",
    "    valid_ds = k_split(num_clients,num_error_clients,dataset['valid'])\n",
    "    dataset = DatasetDict({'train':train_ds[number],'test':test_ds[number],'valid':valid_ds[number]})\n",
    "else:\n",
    "    test_ds = None\n",
    "    valid_ds = k_split(num_clients,num_error_clients,dataset['validation'])\n",
    "    dataset = DatasetDict({'train':train_ds[number],'valid':valid_ds[number]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "Someone just said to me \"with results that are sometimes bracing , sometimes baffling and quite often , and in unexpected ways , touching \".\n",
      "\n",
      "Do you think they are sad or happy?\n"
     ]
    }
   ],
   "source": [
    "# a = dataset['train']['target']\n",
    "# b = dataset['train']['source']\n",
    "# c = Dataset.from_dict({'source':b,'target':a})\n",
    "# print(c['target'])\n",
    "# random.shuffle(a)\n",
    "# from datasets import Dataset\n",
    "# c = Dataset.from_dict({'source':b,'target':a})\n",
    "# print(c['target'])\n",
    "print(dataset['train']['target'][2])\n",
    "print(dataset['train']['source'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1790a0658ff14d8d910d35952a37008d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/182 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # max_length=None => use the model max length (it's actually the default)\n",
    "    model_inputs = tokenizer(examples['source'], truncation=True, max_length=None,padding=True,return_tensors='pt')\n",
    "    if data_name == 'glue':\n",
    "        model_inputs['labels'] = tokenizer(examples['target'], truncation=True, max_length=None,padding=True,return_tensors='pt')[\"input_ids\"]\n",
    "    else:\n",
    "        model_inputs['labels'] = tokenizer([_[0] for _ in examples['target']], truncation=True, max_length=None,padding=True,return_tensors='pt')[\"input_ids\"]\n",
    "    return model_inputs\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, return_dict=True)\n",
    "model_name = model_name_or_path.split(\"/\")[-1]\n",
    "lora_model = PeftModel.from_pretrained(model,f'JsSparkYyx/flan-t5-base-finetuned-lora-{task}-{number}')\n",
    "# lora_model = PeftModel.from_pretrained(model,f'JsSparkYyx/flan-t5-base-finetuned-lora-{task}-1')\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(outputs, ground_truths):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for output, truth in zip(outputs, ground_truths):\n",
    "        if data_name == \"bigbench\":\n",
    "            truth = truth[0]\n",
    "        if output.strip().lower().replace(\".\", \"\") == truth.strip().lower().replace(\".\", \"\"):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "example_predictions = []\n",
    "example_predictions_lora = []\n",
    "# load model\n",
    "\n",
    "# use gpu if available\n",
    "eval_set = \"test\" if data_name == 'glue' else \"valid\"\n",
    "batch_size = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lora_model.to(device)\n",
    "base_model.to(device)\n",
    "with torch.no_grad():\n",
    "    for i in trange(0, len(dataset[eval_set][\"source\"]), batch_size):\n",
    "        inputs = tokenizer(\n",
    "                dataset[eval_set][\"source\"][i : i + batch_size],\n",
    "                max_length=2048,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "            ).to(device)\n",
    "        outputs = base_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"], max_new_tokens=256\n",
    "        )\n",
    "        outputs = tokenizer.batch_decode(\n",
    "            outputs.to(\"cpu\"), skip_special_tokens=True\n",
    "        )\n",
    "        example_predictions.extend(outputs)\n",
    "        outputs = lora_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"], max_new_tokens=256\n",
    "        )\n",
    "        outputs = tokenizer.batch_decode(\n",
    "            outputs.to(\"cpu\"), skip_special_tokens=True\n",
    "        )\n",
    "        example_predictions_lora.extend(outputs)\n",
    "\n",
    "task_perf = accuracy_score(example_predictions, dataset[eval_set][\"target\"])\n",
    "task_perf_lora = accuracy_score(example_predictions_lora, dataset[eval_set][\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.9010989010989\n",
      "56.043956043956044\n"
     ]
    }
   ],
   "source": [
    "print(task_perf)\n",
    "print(task_perf_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5045871559633035\n",
      "29.882044560943644\n"
     ]
    }
   ],
   "source": [
    "print(task_perf)\n",
    "print(task_perf_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the result of the following arithmetic operations?:subtract(26, 2)\\n choice:21\\n choice:24\\n choice:22\\n choice:20\\n choice:23\\nA:', 'What is the answer to the following math word problem?:there are 1200 jelly beans divided between two jars , jar x and jar y . if there are 400 fewer jelly beans in jar x than 3 times the number of beans in jar y , how many beans are in jar x ?\\n choice:700\\n choice:850\\n choice:750\\n choice:650\\n choice:800\\nA:', 'What is the answer to the following math word problem, with the given hint?:6 x – 5 y + 3 z = 22 4 x + 8 y – 11 z = 7 5 x – 6 y + 2 z = 12 given the equations above , x + y + z = ?\\nmultiply(5, 2)\\n choice:12\\n choice:15\\n choice:13\\n choice:10\\n choice:14\\nA:', 'What is the result of the following arithmetic operations?:subtract 15.5 from 15.8,  subtract 15.8 from 16.4,  and then divide both.\\n choice:2 : 1\\n choice:2 : 6\\n choice:2 : 3\\n choice:1 : 2\\n choice:2 : 5\\nA:', 'What is the result of the following arithmetic operations?:divide(add(10, 40), divide(subtract(100, 75), 100))\\n choice:210\\n choice:300\\n choice:225\\n choice:200\\n choice:250\\nA:']\n",
      "['-21.5', '3 * y = 3 * 3 = 12', 'x + y + z = x + y + z + z', '0.5', '225']\n",
      "[['24'], ['800'], ['10'], ['1 : 2'], ['200']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lora_model.to(device)\n",
    "base_model.to(device)\n",
    "inputs = tokenizer(\n",
    "        dataset['valid']['source'][:5],\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(device)\n",
    "outputs = base_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], max_new_tokens=256\n",
    "    )\n",
    "outputs = tokenizer.batch_decode(\n",
    "        outputs.to(\"cpu\"), skip_special_tokens=True\n",
    "    )\n",
    "print(dataset['valid']['source'][:5])\n",
    "print(outputs)\n",
    "print(dataset['valid']['target'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[363, 19, 8, 741, 13, 8, 826, 3, 9, 30922, 51, 7578, 2673, 58, 10, 23829, 102, 120, 1902, 1714, 12, 11558, 14514, 741, 57, 3, 17225, 6, 1160, 10, 11434, 1160, 10, 16975, 1160, 10, 5426, 1160, 10, 15239, 1160, 10, 17225, 71, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['input_ids'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\WashU\\NLP524Final\\evaluation.ipynb 单元格 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/WashU/NLP524Final/evaluation.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/WashU/NLP524Final/evaluation.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49mdataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39msource\u001b[39;49m\u001b[39m'\u001b[39;49m][:\u001b[39m5\u001b[39;49m], max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/WashU/NLP524Final/evaluation.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     )\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\LLM\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\generation\\utils.py:1511\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[39m# 3. Define model inputs\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_model_inputs(\n\u001b[0;32m   1509\u001b[0m     inputs, generation_config\u001b[39m.\u001b[39mbos_token_id, model_kwargs\n\u001b[0;32m   1510\u001b[0m )\n\u001b[1;32m-> 1511\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1513\u001b[0m \u001b[39m# 4. Define other model kwargs\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39moutput_attentions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39moutput_attentions\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "        input_ids=dataset['train']['source'][:5], max_new_tokens=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes', 'yes', 'no', 'no', 'no']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tokenized_datasets['train']['labels'][:5]\n",
    "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "tokenizer.batch_decode(labels, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
